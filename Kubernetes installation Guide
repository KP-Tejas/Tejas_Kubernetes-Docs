Step1:
We should always have 1 master node and 2 worker nodes(they should be off 2GB RAM and 2GB CPU)

*****************Port Mapping***************
Link of Kubernetes.io ----- https://kubernetes.io/docs/reference/networking/ports-and-protocols/

On Master node:
Protocol	Direction	PortRange	  Purpose	                 Used By
TCP	     Inbound	6443	        Kubernetes               API server	All
TCP	     Inbound	2379-2380	    etcd server client API	 kube-apiserver, etcd
TCP	     Inbound	10250	        Kubelet API	             Self, Control plane
TCP	     Inbound	10259        	kube-scheduler	         Self
TCP	     Inbound	10257        	kube-controller-manager	 Self
TCP      Inbound  6783          Weave-net                Self

On Worker nodes:
Protocol	Direction	    Port Range	   Purpose	            Used By
TCP	      Inbound	      10250	         Kubelet API	        Self, Control plane
TCP	      Inbound	      30000-32767	   NodePort Services   	All
TCP      Inbound        6783           Weave-net            Self



***********************On Both master and Woker nodes********************
::::::::::::::
Step 1: sudo swapoff -a
::::::::::::::: 
Step 2: Edit the /etc/hosts (All three nodes)
        sudo vim /etc/hosts(IP  address will be internal IP's)

172.31.41.15  Master
172.31.44.82  Worker 1
172.31.39.242 Worker 2

Step3 ::::::::::::Change the host name of the system (All three nodes based on Host)
sudo hostnamectl set-hostname Master(Later exit and Restart the session)

Step4::::::::::::::::Container Run-time interface(Forwarding IPv4 and letting iptables see bridged traffic)

cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
overlay
br_netfilter
EOF

sudo modprobe overlay
sudo modprobe br_netfilter

# sysctl params required by setup, params persist across reboots
cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
net.bridge.bridge-nf-call-iptables  = 1
net.bridge.bridge-nf-call-ip6tables = 1
net.ipv4.ip_forward                 = 1
EOF

# Apply sysctl params without reboot
sudo sysctl --system

Step5::::::::::::::::
# updating the service and installing the containerd
sudo apt update -y 
sudo apt-get install -y containerd

#Set up the default configuration file.

sudo mkdir -p /etc/containerd
sudo containerd config default | sudo tee /etc/containerd/config.toml

#Next up, we need to modify the containerd configuration file and ensure that the cgroupDriver is set to systemd. To do so, #edit the following file:

sudo vi /etc/containerd/config.toml

#Scroll down to the following section:

#[plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
#And ensure that value of SystemdCgroup is set to true Make sure the contents of your section match the following:

#[plugins."io.containerd.grpc.v1.cri".containerd.runtimes.runc.options]
#    BinaryName = ""
#    CriuImagePath = ""
#    CriuPath = ""
#    CriuWorkPath = ""
#    IoGid = 0
#    IoUid = 0
#    NoNewKeyring = false
#    NoPivotRoot = false
#    Root = ""
#    ShimCgroup = ""
##    SystemdCgroup = true

#Finally, to apply these changes, we need to restart containerd.

sudo systemctl restart containerd

#To check that containerd is indeed running, use this command:

ps -ef | grep containerd

#Expect output similar to this:

#root       63087       1  0 13:16 ?        00:00:00 /usr/bin/containerd

#To check the status of Containerd
service containerd status

# Installation of kubeadm kubelet and kubectl

sudo apt-get update
# apt-transport-https may be a dummy package; if so, you can skip that package
sudo apt-get install -y apt-transport-https ca-certificates curl gpg

# If the directory `/etc/apt/keyrings` does not exist, it should be created before the curl command, read the note below.
# sudo mkdir -p -m 755 /etc/apt/keyrings
curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.29/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg

# This overwrites any existing configuration in /etc/apt/sources.list.d/kubernetes.list
echo 'deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.29/deb/ /' | sudo tee /etc/apt/sources.list.d/kubernetes.list


sudo apt-get update
sudo apt-get install -y kubelet kubeadm kubectl
sudo apt-mark hold kubelet kubeadm kubectl


**************If u need a specific version of kubeadm then use this command apt-cache madison kubeadm***************
then u can specify sudo apt-get install -y kubelet=1.29.3-1.1 kubeadm=1.29.3-1.1 kubelet=1.29.3-1.1

Note---- u have to install same version of kubeadm kubelet and kubectl

***************************************Only on Master node*************************
Step1----Sudo kubeadm init

Check the nodes on master node
sudo kubectl get node --kubeconfig /etc/kubernetes/admin.conf


:::::::::::::::::::::Copy this on Master node and execute this:::::::::::::::::::::::::::

************Setting the kubeconfig file as enviornment variable******************** fileAlternatively, if you are the root user, you can run:
  sudo -i
  export KUBECONFIG=/etc/kubernetes/admin.conf

************************we are copying the config in default mode******************
  mkdir -p $HOME/.kube
  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
  sudo chown $(id -u):$(id -g) $HOME/.kube/config

****************************Weave-net installation*******************

Step1: wget https://github.com/weaveworks/weave/releases/download/v2.8.1/weave-daemonset-k8s.yaml -O weave.yml

edit the file vim weave.yml
 file looks like:::::::::::::::
containers:
            - name: weave
              command:
                - /home/weave/launch.sh
                - --ipalloc-range=100.32.0.0/12

To apply the file then just apply kubectl apply -f weave.yml
::::::::::::::::::::::::: Pod network plugin was pre-requisite for joining the the worker node::::::::::


:::Add the worker node now:::
sudo kubeadm join 172.31.34.251:6443 --token iz0bb9.e34w2waojd3erbf5 \
        --discovery-token-ca-cert-hash sha256:7247201bde0bc5c0fbdfb2080e5faafe05f91896e9baeb6fdc222e67986e8da4


If u lost the kubeadm join command then use *******************kubeadm token create --print-join-command************************




